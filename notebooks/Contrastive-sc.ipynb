{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original implementation of Contrastive-sc method\n",
    "(https://github.com/ciortanmadalina/contrastive-sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn  -U # AttributeError: 'str' object has no attribute 'decode' in fitting Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import argparse\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import st_loss\n",
    "\n",
    "import h5py\n",
    "import scipy as sp\n",
    "import scanpy.api as sc\n",
    "from collections import Counter\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import train\n",
    "import os\n",
    "import glob2\n",
    "import interpret_utils\n",
    "plt.ion()\n",
    "plt.show()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../\"\n",
    "category = \"balanced_data\"\n",
    "category = \"imbalanced_data\"\n",
    "files = glob2.glob(f'{path}R/simulated_data/{category}/*.h5')\n",
    "files = [f[len(f\"{path}R/simulated_data/{category}/\"):-3] for f in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"contrastivesc\"\n",
    "run = 0\n",
    "nb_features = -1\n",
    "pval_cutoff = None\n",
    "dropout = 0.9\n",
    "lr = 0.4\n",
    "layers = [200, 40, 60]\n",
    "temperature = 0.07\n",
    "for category in [ \"balanced_data\", \"imbalanced_data\",\n",
    "                ]:\n",
    "    files = glob2.glob(f'{path}R/simulated_data/{category}/*.h5')\n",
    "    files = [f[len(f\"{path}R/simulated_data/{category}/\"):-3] for f in files]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for dataset in files:\n",
    "        print(f\">>>>> Data {dataset}\")\n",
    "        t0 = time.time()\n",
    "        data_mat = h5py.File(f\"{path}R/simulated_data/{category}/{dataset}.h5\",\n",
    "                             \"r\")\n",
    "        print(f\"Dropout {data_mat['dropout'][()][0]}\")\n",
    "        X = np.array(data_mat['X'])\n",
    "        Y = np.array(data_mat['Y'])\n",
    "        cluster_number = np.unique(Y).shape[0]\n",
    "        nb_genes = 1500\n",
    "        X, idx = train.preprocess(X, nb_genes=nb_genes, return_idx=True)\n",
    "\n",
    "        torch.manual_seed(run)\n",
    "        torch.cuda.manual_seed_all(run)\n",
    "        np.random.seed(run)\n",
    "        random.seed(run)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        start = time.time()\n",
    "        dresults, model = train.run(X,\n",
    "                                    cluster_number,\n",
    "                                    dataset,\n",
    "                                    Y=Y,\n",
    "                                    nb_epochs=300,\n",
    "                                    lr=lr,\n",
    "                                    temperature=temperature,\n",
    "                                    dropout=dropout,\n",
    "                                    layers=layers,\n",
    "                                    save_to=f\"{path}output/{category}/{run}/\",\n",
    "                                    save_pred=True)\n",
    "        clusters = dresults[\"kmeans_pred\"]\n",
    "        interpret_utils.de_analysis([X, np.array(data_mat['X'])],\n",
    "                                    [\"proc_\",  \"full_\"],\n",
    "                                    data_mat,\n",
    "                                    idx,\n",
    "                                    method,\n",
    "                                    dataset,\n",
    "                                    category,\n",
    "                                    clusters,\n",
    "                                    nb_features=nb_features,\n",
    "                                    run=run,\n",
    "                                    pval_cutoff=pval_cutoff)\n",
    "        folder = f\"../output/interpretability/{category}/{method}\"\n",
    "        write_to = f\"{folder}/{dataset}\"\n",
    "\n",
    "        with open(f\"{write_to}_all.pkl\", 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        start = time.time()\n",
    "        device = train.get_device()\n",
    "        criterion_rep = st_loss.SupConLoss(temperature=temperature)\n",
    "\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                            model.parameters()),\n",
    "                                     lr=lr)\n",
    "\n",
    "        results[\"features\"][\"saliency\"] = []\n",
    "        results[\"features\"][\"grad_x_input\"] = []\n",
    "\n",
    "        stot = pd.DataFrame()\n",
    "        model.train()\n",
    "        grad_time = 0\n",
    "        for c in np.sort(np.unique(results[\"meta\"][\"clusters\"])):\n",
    "            ii = np.where(results[\"meta\"][\"clusters\"] == c)[0]\n",
    "            input1 = torch.FloatTensor(X[ii]).to(device)\n",
    "            input2 = torch.FloatTensor(X[ii]).to(device)\n",
    "            input1.requires_grad = True\n",
    "            input2.requires_grad = True\n",
    "            anchors_output = model(input1, dropout = 0)\n",
    "            neighbors_output = model(input2, dropout = 0)\n",
    "\n",
    "            features = torch.cat(\n",
    "                [anchors_output.unsqueeze(1),\n",
    "                 neighbors_output.unsqueeze(1)],\n",
    "                dim=1)\n",
    "            total_loss = criterion_rep(features)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            g1 = np.mean(np.array([np.abs(input1.grad.cpu().numpy()), \n",
    "                                   np.abs(input2.grad.cpu().numpy())]), axis = 0)\n",
    "        #     w1 = model.encoder[1].weight.detach().cpu().numpy()\n",
    "        #     g1 = model.encoder[1].weight.grad.detach().cpu().numpy()\n",
    "        #     wa1 = (w1*g1).mean(0)\n",
    "        #     wa1 = wa1.mean(1)\n",
    "        #     print(model.encoder[1].weight.grad.shape, X[ii].shape)\n",
    "        #     saliency = np.abs(g1.mean(0))\n",
    "            t1 = time.time()\n",
    "            grad_x_input = g1 * X[ii]\n",
    "            grad_x_input = grad_x_input.mean(0)\n",
    "            t2 = time.time()\n",
    "            grad_time += (t2-t1)\n",
    "            saliency = np.abs(g1.mean(0))\n",
    "        #     saliency = np.mean(np.abs(g1), axis = 0)\n",
    "\n",
    "            s_r= np.argsort(saliency)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"saliency\"].append(s_r)\n",
    "            s = pd.DataFrame()\n",
    "            s[\"x\"] = s_r\n",
    "            s[\"saliency\"] = np.sort(saliency)[::-1][:nb_features]\n",
    "            s[\"cluster\"] = c\n",
    "#             print(\">> saliency \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], s_r)))\n",
    "            t1 = time.time()\n",
    "            gi_r= np.argsort(grad_x_input)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"grad_x_input\"].append(gi_r)\n",
    "            gi = pd.DataFrame()\n",
    "            gi[\"x\"] = gi_r\n",
    "            gi[\"grad_x_input\"] = np.sort(grad_x_input)[::-1][:nb_features]\n",
    "            gi[\"cluster\"] = c\n",
    "            t2 =time.time()\n",
    "            grad_time += (t2-t1)\n",
    "#             print(\">> gi \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], gi_r)))\n",
    "        end = time.time()\n",
    "        results[\"time\"][f\"saliency\"] = end - start - grad_time\n",
    "        results[\"time\"][f\"grad_x_input\"] = end - start\n",
    "        s[\"rank\"] = s.groupby(\"cluster\")[\"saliency\"].rank(\"dense\", ascending=False)\n",
    "        gi[\"rank\"] = gi.groupby(\"cluster\")[\"grad_x_input\"].rank(\"dense\", ascending=False)\n",
    "\n",
    "        results[\"scores\"][\"saliency\"] = s\n",
    "        results[\"scores\"][\"grad_x_input\"] = gi\n",
    "        with open(f\"{write_to}_all.pkl\", 'wb') as f:\n",
    "            pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
