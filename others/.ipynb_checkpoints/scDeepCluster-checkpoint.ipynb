{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ankurtaly/Integrated-Gradients/blob/master/IntegratedGradients/integrated_gradients.py\n",
    "\n",
    "https://github.com/TianhongDai/integrated-gradient-pytorch/blob/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set recommended library versions\n",
    "# !pip install tensorflow==1.15.0\n",
    "# !pip install keras==2.1.4\n",
    "# !pip install pandas==1.0.4\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run scDeepCluster on the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scanpy/api/__init__.py:7: FutureWarning: \n",
      "\n",
      "In a future version of Scanpy, `scanpy.api` will be removed.\n",
      "Simply use `import scanpy as sc` and `import scanpy.external as sce` instead.\n",
      "\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part implements the scDeepCluster algoritm\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import integrated_gradients\n",
    "import interpret_utils\n",
    "from time import time\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input, GaussianNoise, Layer, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import h5py\n",
    "import scanpy.api as sc\n",
    "from scDeepCluster_layers import ConstantDispersionLayer, SliceLayer, ColWiseMultLayer\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, calinski_harabasz_score\n",
    "from scDeepCluster_loss import poisson_loss, NB, ZINB\n",
    "from scDeepCluster_preprocess import read_dataset, normalize\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(2211)\n",
    "# tf.random.set_seed(2211)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2211)\n",
    "\n",
    "MeanAct = lambda x: tf.clip_by_value(K.exp(x), 1e-5, 1e6)\n",
    "DispAct = lambda x: tf.clip_by_value(tf.nn.softplus(x), 1e-4, 1e4)\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def autoencoder(dims, noise_sd=0, init='glorot_uniform', act='relu'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        Model of autoencoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    sf_layer = Input(shape=(1,), name='size_factors')\n",
    "    x = Input(shape=(dims[0],), name='counts')\n",
    "    h = x\n",
    "    h = GaussianNoise(noise_sd, name='input_noise')(h)\n",
    " \n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "        h = GaussianNoise(noise_sd, name='noise_%d' % i)(h)    # add Gaussian noise\n",
    "        h = Activation(act)(h)\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_hidden')(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        h = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(h)\n",
    "\n",
    "    # output\n",
    " \n",
    "    pi = Dense(dims[0], activation='sigmoid', kernel_initializer=init, name='pi')(h)\n",
    "\n",
    "    disp = Dense(dims[0], activation=DispAct, kernel_initializer=init, name='dispersion')(h)\n",
    "\n",
    "    mean = Dense(dims[0], activation=MeanAct, kernel_initializer=init, name='mean')(h)\n",
    "\n",
    "    output = ColWiseMultLayer(name='output')([mean, sf_layer])\n",
    "    output = SliceLayer(0, name='slice')([output, disp, pi])\n",
    "\n",
    "    return Model(inputs=[x, sf_layer], outputs=output)\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape = (self.n_clusters, input_dim), \n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "class SCDeepCluster(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 noise_sd=0,\n",
    "                 alpha=1.0,\n",
    "                 ridge=0,\n",
    "                 debug=False):\n",
    "\n",
    "        super(SCDeepCluster, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.noise_sd = noise_sd\n",
    "        self.alpha = alpha\n",
    "        self.act = 'relu'\n",
    "        self.ridge = ridge\n",
    "        self.debug = debug\n",
    "        self.autoencoder = autoencoder(self.dims, noise_sd=self.noise_sd, act = self.act)\n",
    "        \n",
    "        # prepare clean encode model without Gaussian noise\n",
    "        ae_layers = [l for l in self.autoencoder.layers]\n",
    "        hidden = self.autoencoder.input[0]\n",
    "        for i in range(1, len(ae_layers)):\n",
    "            if \"noise\" in ae_layers[i].name:\n",
    "                next\n",
    "            elif \"dropout\" in ae_layers[i].name:\n",
    "                next\n",
    "            else:\n",
    "                hidden = ae_layers[i](hidden)\n",
    "            if \"encoder_hidden\" in ae_layers[i].name:  # only get encoder layers\n",
    "                break\n",
    "        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n",
    "\n",
    "        pi = self.autoencoder.get_layer(name='pi').output\n",
    "        disp = self.autoencoder.get_layer(name='dispersion').output\n",
    "        mean = self.autoencoder.get_layer(name='mean').output\n",
    "        zinb = ZINB(pi, theta=disp, ridge_lambda=self.ridge, debug=self.debug)\n",
    "        self.loss = zinb.loss\n",
    "\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, alpha=self.alpha, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=[self.autoencoder.input[0], self.autoencoder.input[1]],\n",
    "                           outputs=[clustering_layer, self.autoencoder.output])\n",
    "\n",
    "        self.pretrained = False\n",
    "        self.centers = []\n",
    "        self.y_pred = []\n",
    "\n",
    "    def pretrain(self, x, y, batch_size=256, epochs=200, optimizer='adam', ae_file='ae_weights.h5'):\n",
    "        print('...Pretraining autoencoder...')\n",
    "        self.autoencoder.compile(loss=self.loss, optimizer=optimizer)\n",
    "        es = EarlyStopping(monitor=\"loss\", patience=50, verbose=1)\n",
    "        self.autoencoder.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, callbacks=[es], verbose = 0)\n",
    "        self.autoencoder.save_weights(ae_file)\n",
    "        print('Pretrained weights are saved to ./' + str(ae_file))\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):  # load weights of scDeepCluster model\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict_clusters(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution P which enhances the discrimination of soft label Q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def fit(self, x_counts, sf, y, raw_counts, batch_size=256, maxiter=2e4, tol=1e-3, update_interval=140,\n",
    "            ae_weights=None, save_dir='./output/pickle_results/scDeepCluster', loss_weights=[1,1], optimizer='adadelta'):\n",
    "\n",
    "        self.model.compile(loss=['kld', self.loss], loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = int(x_counts.shape[0] / batch_size) * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: pretrain\n",
    "        if not self.pretrained and ae_weights is None:\n",
    "            print('...pretraining autoencoders using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=200')\n",
    "            self.pretrain(x, batch_size)\n",
    "            self.pretrained = True\n",
    "        elif ae_weights is not None:\n",
    "            self.autoencoder.load_weights(ae_weights)\n",
    "            print('ae_weights is loaded successfully.')\n",
    "\n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict([x_counts, sf]))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/scDeepCluster_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = [0, 0, 0]\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict([x_counts, sf], verbose=0)\n",
    "                self.features = self.extract_feature([x_counts, sf])\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "#                     acc = np.round(cluster_acc(y, self.y_pred), 5)\n",
    "                    nmi = np.round(metrics.normalized_mutual_info_score(y, self.y_pred), 5)\n",
    "                    ari = np.round(metrics.adjusted_rand_score(y, self.y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    \n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x_counts.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=[x_counts[index * batch_size::], sf[index * batch_size:]],\n",
    "                                                 y=[p[index * batch_size::], raw_counts[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=[x_counts[index * batch_size:(index + 1) * batch_size], \n",
    "                                                    sf[index * batch_size:(index + 1) * batch_size]],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    raw_counts[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "#             if ite % save_interval == 0:\n",
    "#                 # save scDeepCluster model checkpoints\n",
    "#                 print('saving model to: ' + save_dir + '/scDeepCluster_model_' + str(ite) + '.h5')\n",
    "#                 self.model.save_weights(save_dir + '/scDeepCluster_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to: ' + save_dir + '/scDeepCluster_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/scDeepCluster_model_final.h5')\n",
    "        \n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(scDeepCluster, adata):\n",
    "    from keras import backend as k\n",
    "\n",
    "    outputTensor = scDeepCluster.model.output\n",
    "    listOfVariableTensors = scDeepCluster.model.trainable_weights[0]\n",
    "    gradients = k.gradients(outputTensor, listOfVariableTensors)\n",
    "\n",
    "#     init = tf.global_variables_initializer()\n",
    "#     sess = tf.Session()\n",
    "#     sess.run(init)\n",
    "    sess = k.get_session()\n",
    "\n",
    "    evaluated_gradients = []\n",
    "    for i in range(len(adata.X)):\n",
    "        idx = np.array([i])\n",
    "        ii=[adata.X[idx], adata.obs.size_factors[idx].values.reshape(-1, 1)]\n",
    "        g = sess.run(gradients[0],\n",
    "                                   feed_dict={\n",
    "                                       scDeepCluster.model.input[0]: ii[0],\n",
    "                                       scDeepCluster.model.input[1]: ii[1]\n",
    "                                   })\n",
    "        evaluated_gradients.append(g.mean(axis=1))\n",
    "    evaluated_gradients = np.array(evaluated_gradients).reshape(len(adata.X), -1)\n",
    "    return evaluated_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import glob2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 9)\n",
      ">>>>dataset data_0c_2de_0.3, x: (500, 2500)\n",
      "### Autoencoder: Successfully preprocessed 2500 genes and 500 cells.\n",
      "Sample size\n",
      "(500, 2472)\n",
      "(500,)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:126: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3805: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "...Pretraining autoencoder...\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/optimizers.py:752: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/interpretability/others/scDeepCluster_loss.py:87: The name tf.lgamma is deprecated. Please use tf.math.lgamma instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/interpretability/others/scDeepCluster_loss.py:88: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/interpretability/others/scDeepCluster_loss.py:10: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/interpretability/others/scDeepCluster_loss.py:10: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:960: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2496: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:196: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Pretrained weights are saved to ./ae_weights.h5\n",
      "Update interval 1\n",
      "Save interval 5\n",
      "Initializing cluster centers with k-means.\n",
      "delta_label  0.0 < tol  0.001\n",
      "Reached tolerance threshold. Stopping training.\n",
      "saving model to: scDeepCluster/scDeepCluster_model_final.h5\n",
      "ARI before: 1.0\n",
      "Max nb_features 771\n",
      "ARI after: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:10:12] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:11:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (500, 2472)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:11:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:12:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (500, 2500)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b97a4253ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                     \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                                     pval_cutoff=pval_cutoff)\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mevaluated_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscDeepCluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"../output/interpretability/{category}/{method}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'time'"
     ]
    }
   ],
   "source": [
    "method = \"scDeepCluster\"\n",
    "nb_features = 100\n",
    "pval_cutoff=None\n",
    "run = 0\n",
    "for category in [\"balanced_data\",\"imbalanced_data\" #, \"balanced_data\",\n",
    "#                  \"real_data\", \"imbalanced_data\"\n",
    "                ]:\n",
    "\n",
    "    path = \"..\"\n",
    "    if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "        files = glob2.glob(f'{path}/R/simulated_data/{category}/*.h5')\n",
    "        files = [\n",
    "            f[len(f\"{path}/R/simulated_data/{category}/\"):-3] for f in files\n",
    "        ]\n",
    "    else:\n",
    "        files = glob2.glob(f'{path}/real_data/*.h5')\n",
    "        files = [f[len(f\"{path}/real_data/\"):-3] for f in files]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"ARI\", \"NMI\", \"sil\", \"run\", \"time\", \"pred\", \"cal\", \"features\"])\n",
    "#     df = pd.read_pickle(\n",
    "#                 f\"../output/pickle_results/{category}/{category}_scDeepCluster.pkl\"\n",
    "#             )\n",
    "    print(df.shape)\n",
    "    for dataset in files:\n",
    "        if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "            data_mat = h5py.File(\n",
    "                f\"{path}/R/simulated_data/{category}/{dataset}.h5\", \"r\")\n",
    "\n",
    "        else:\n",
    "            data_mat = h5py.File(f\"{path}/real_data/{dataset}.h5\", \"r\")\n",
    "\n",
    "        y = np.array(data_mat['Y'])\n",
    "        x = np.array(data_mat['X'])\n",
    "        print(f\">>>>dataset {dataset}, x: {x.shape}\")\n",
    "\n",
    "\n",
    "        start = time()\n",
    "        #### Run scDeepCluster on the simulated data\n",
    "        x = np.ceil(x).astype(np.int)\n",
    "        optimizer1 = Adam(amsgrad=True)\n",
    "        optimizer2 = 'adadelta'\n",
    "\n",
    "        # preprocessing scRNA-seq read counts matrix\n",
    "        adata = sc.AnnData(x)\n",
    "        adata.obs['Group'] = y\n",
    "\n",
    "        adata = read_dataset(adata,\n",
    "                             transpose=False,\n",
    "                             test_split=False,\n",
    "                             copy=True)\n",
    "\n",
    "        adata = normalize(adata,\n",
    "                          size_factors=True,\n",
    "                          normalize_input=True,\n",
    "                          logtrans_input=True)\n",
    "        idx = adata.var_names.astype(int).tolist()\n",
    "\n",
    "\n",
    "        input_size = adata.n_vars\n",
    "\n",
    "        print('Sample size')\n",
    "        print(adata.X.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "        x_sd = adata.X.std(0)\n",
    "        x_sd_median = np.median(x_sd)\n",
    "\n",
    "        update_interval = int(adata.X.shape[0] / 256)\n",
    "\n",
    "        seed = run\n",
    "        np.random.seed(seed)\n",
    "        # Define scDeepCluster model\n",
    "        scDeepCluster = SCDeepCluster(dims=[input_size, 256, 64, 32],\n",
    "                                      n_clusters=np.unique(y).shape[0],\n",
    "                                      noise_sd=2.5)\n",
    "\n",
    "        t0 = time()\n",
    "        # Pretrain autoencoders before clustering\n",
    "        scDeepCluster.pretrain(x=[adata.X, adata.obs.size_factors],\n",
    "                               y=adata.raw.X,\n",
    "                               batch_size=256,\n",
    "                               epochs=600,\n",
    "                               optimizer=optimizer1,\n",
    "                               ae_file='ae_weights.h5')\n",
    "\n",
    "        # begin clustering, time not include pretraining part.\n",
    "\n",
    "        gamma = 1.  # set hyperparameter gamma\n",
    "        scDeepCluster.fit(\n",
    "            x_counts=adata.X,\n",
    "            sf=adata.obs.size_factors,\n",
    "            y=y,\n",
    "            raw_counts=adata.raw.X,\n",
    "            batch_size=256,\n",
    "            tol=0.001,\n",
    "            maxiter=20000,\n",
    "            update_interval=update_interval,\n",
    "            ae_weights=None,\n",
    "            save_dir='scDeepCluster',\n",
    "            loss_weights=[gamma, 1],\n",
    "            optimizer=optimizer2)\n",
    "\n",
    "        # Show the final results\n",
    "        clusters = scDeepCluster.y_pred\n",
    "\n",
    "        interpret_utils.de_analysis([adata.X, np.array(data_mat['X'])],\n",
    "                                    [\"proc_\", \"full_\"],\n",
    "                                    data_mat,\n",
    "                                    idx,\n",
    "                                    method,\n",
    "                                    dataset,\n",
    "                                    category,\n",
    "                                    clusters,\n",
    "                                    nb_features=nb_features,\n",
    "                                    run=run,\n",
    "                                    pval_cutoff=pval_cutoff)\n",
    "        start = time.time()\n",
    "        evaluated_gradients = get_gradients(scDeepCluster, adata)\n",
    "        folder = f\"../output/interpretability/{category}/{method}\"\n",
    "        write_to = f\"{folder}/{dataset}\"\n",
    "\n",
    "\n",
    "        with open(f\"{write_to}_all.pkl\", 'rb') as f:\n",
    "            results= pickle.load(f)\n",
    "\n",
    "        results[\"features\"][\"saliency\"] = []\n",
    "        results[\"features\"][\"grad_x_input\"] = []\n",
    "        stot = pd.DataFrame()\n",
    "        for c in np.sort(np.unique(results[\"meta\"][\"clusters\"])):\n",
    "            ii = np.where(results[\"meta\"][\"clusters\"] ==c)[0]\n",
    "        #     g1 = evaluated_gradients[ii] * adata.X[ii]\n",
    "        #     g1 = g1.mean(axis = 0)\n",
    "            g1 = evaluated_gradients[ii].mean(axis = 0)\n",
    "            grad_x_input = evaluated_gradients[ii] * adata.X[ii]\n",
    "            grad_x_input = grad_x_input.mean(0)\n",
    "            saliency = np.abs(g1)\n",
    "        #     saliency = g1\n",
    "\n",
    "            s_r= np.argsort(saliency)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"saliency\"].append(s_r)\n",
    "            s = pd.DataFrame()\n",
    "            s[\"x\"] = s_r\n",
    "            s[\"saliency\"] = np.sort(saliency)[::-1][:nb_features]\n",
    "            s[\"cluster\"] = c\n",
    "            print(\">> saliency \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], s_r)))\n",
    "\n",
    "            gi_r= np.argsort(grad_x_input)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"grad_x_input\"].append(gi_r)\n",
    "            gi = pd.DataFrame()\n",
    "            gi[\"x\"] = gi_r\n",
    "            gi[\"grad_x_input\"] = np.sort(grad_x_input)[::-1][:nb_features]\n",
    "            gi[\"cluster\"] = c\n",
    "            print(\">> gi \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], gi_r)))\n",
    "        s[\"rank\"] = s.groupby(\"cluster\")[\"saliency\"].rank(\"dense\", ascending=False)\n",
    "        gi[\"rank\"] = gi.groupby(\"cluster\")[\"grad_x_input\"].rank(\"dense\", ascending=False)\n",
    "        results[\"scores\"][\"saliency\"] = s\n",
    "        results[\"scores\"][\"grad_x_input\"] = gi\n",
    "        end = time.time()\n",
    "        results[\"time\"][f\"saliency\"] = end - start - grad_time\n",
    "        results[\"time\"][f\"grad_x_input\"] = end - start\n",
    "        with open(f\"{write_to}_all.pkl\", 'wb') as f:\n",
    "            pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
