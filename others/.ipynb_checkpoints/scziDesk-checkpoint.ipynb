{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15.0\n",
    "\n",
    "# !pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pickle\n",
    "import interpret_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from scziDesk_preprocess import *\n",
    "from scziDesk_network import *\n",
    "from scziDesk_utils import *\n",
    "import argparse\n",
    "import h5py\n",
    "import time\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, calinski_harabasz_score\n",
    "from collections import Counter\n",
    "import glob2\n",
    "plt.ion()\n",
    "plt.show()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = [1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999, 10000]\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"train\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# parser.add_argument(\"--dataname\", default = \"Quake_10x_Bladder\", type = str)\n",
    "parser.add_argument(\"--dataname\", default = \"data_-1.5c15\", type = str)\n",
    "\n",
    "parser.add_argument(\"--distribution\", default = \"ZINB\")\n",
    "parser.add_argument(\"--self_training\", default = True)\n",
    "parser.add_argument(\"--dims\", default = [499, 256, 64, 32])\n",
    "parser.add_argument(\"--highly_genes\", default = 500)\n",
    "parser.add_argument(\"--alpha\", default = 0.001, type = float)\n",
    "parser.add_argument(\"--gamma\", default = 0.001, type = float)\n",
    "parser.add_argument(\"--learning_rate\", default = 0.0001, type = float)\n",
    "parser.add_argument(\"--random_seed\", default = random_seed)\n",
    "parser.add_argument(\"--batch_size\", default = 256, type = int)\n",
    "parser.add_argument(\"--update_epoch\", default = 10, type = int)\n",
    "parser.add_argument(\"--pretrain_epoch\", default = 1000, type = int)\n",
    "parser.add_argument(\"--funetrain_epoch\", default = 2000, type = int)\n",
    "parser.add_argument(\"--t_alpha\", default = 1.0)\n",
    "parser.add_argument(\"--noise_sd\", default = 1.5)\n",
    "parser.add_argument(\"--error\", default = 0.001, type = float)\n",
    "parser.add_argument(\"--gpu_option\", default = \"0\")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_0c_2de_0.3', 'data_-1c_2de_0.05', 'data_0c_2de_0.1', 'data_-1c_3de_0.1', 'data_0c_3de_0.05', 'data_-1c_2de_0.1', 'data_1c_2de_0.1', 'data_0c_3de_0.3', 'data_1c_3de_0.1', 'data_-1c_2de_0.3', 'data_1c_2de_0.05', 'data_-1c_3de_0.3', 'data_1c_2de_0.3', 'data_1c_3de_0.05', 'data_-1c_3de_0.05', 'data_0c_2de_0.05', 'data_0c_3de_0.1', 'data_1c_3de_0.3']\n",
      ">>>>dataset data_0c_2de_0.3\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 500) (3000, 500)\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:30: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:34: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_loss.py:32: The name tf.lgamma is deprecated. Please use tf.math.lgamma instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_loss.py:33: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_loss.py:12: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_loss.py:12: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:79: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:92: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "begin the pretraining\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:98: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:98: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:101: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:104: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py:141: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "begin the funetraining\n",
      "ARI 1.0, NMI 1.0\n",
      "0 ARI 1.0, NMI 1.0\n",
      "ARI before: 1.0\n",
      "Max nb_features 181\n",
      "ARI after: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:34:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (3000, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:34:41] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (3000, 2500)\n",
      ">>>>dataset data_-1c_2de_0.05\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 499) (3000, 499)\n",
      "begin the pretraining\n",
      "begin the funetraining\n",
      "ARI 0.56904, NMI 0.47497\n",
      "0 ARI 0.57, NMI 0.47487\n",
      "10 ARI 0.56175, NMI 0.46337\n",
      "20 ARI 0.56572, NMI 0.4663\n",
      "30 ARI 0.56975, NMI 0.47004\n",
      "40 ARI 0.5707, NMI 0.47\n",
      "50 ARI 0.56875, NMI 0.4693\n",
      "60 ARI 0.57275, NMI 0.47227\n",
      "70 ARI 0.57075, NMI 0.47078\n",
      "80 ARI 0.57175, NMI 0.47152\n",
      "90 ARI 0.5707, NMI 0.47\n",
      "100 ARI 0.56975, NMI 0.47004\n",
      "110 ARI 0.5707, NMI 0.47\n",
      "120 ARI 0.56879, NMI 0.47008\n",
      "130 ARI 0.57179, NMI 0.47231\n",
      "140 ARI 0.57584, NMI 0.47608\n",
      "150 ARI 0.57881, NMI 0.47756\n",
      "160 ARI 0.57889, NMI 0.47913\n",
      "170 ARI 0.58293, NMI 0.48216\n",
      "180 ARI 0.58495, NMI 0.48368\n",
      "190 ARI 0.58697, NMI 0.48521\n",
      "200 ARI 0.58803, NMI 0.48677\n",
      "210 ARI 0.589, NMI 0.48675\n",
      "220 ARI 0.59214, NMI 0.49066\n",
      "230 ARI 0.59104, NMI 0.4883\n",
      "240 ARI 0.59307, NMI 0.48985\n",
      "250 ARI 0.5952, NMI 0.49299\n",
      "260 ARI 0.59618, NMI 0.49298\n",
      "270 ARI 0.5963, NMI 0.49537\n",
      "280 ARI 0.59831, NMI 0.49613\n",
      "290 ARI 0.59945, NMI 0.49935\n",
      "300 ARI 0.60348, NMI 0.50087\n",
      "310 ARI 0.60368, NMI 0.505\n",
      "320 ARI 0.60876, NMI 0.50729\n",
      "330 ARI 0.60689, NMI 0.50996\n",
      "340 ARI 0.61822, NMI 0.51703\n",
      "350 ARI 0.61114, NMI 0.51579\n",
      "360 ARI 0.6257, NMI 0.52618\n",
      "370 ARI 0.62065, NMI 0.52666\n",
      "380 ARI 0.62573, NMI 0.52706\n",
      "390 ARI 0.62383, NMI 0.53002\n",
      "400 ARI 0.63533, NMI 0.53718\n",
      "410 ARI 0.62911, NMI 0.53503\n",
      "420 ARI 0.63972, NMI 0.54424\n",
      "430 ARI 0.6334, NMI 0.54024\n",
      "440 ARI 0.6429, NMI 0.54676\n",
      "450 ARI 0.64298, NMI 0.54868\n",
      "460 ARI 0.64824, NMI 0.55099\n",
      "470 ARI 0.65049, NMI 0.55557\n",
      "480 ARI 0.65689, NMI 0.55975\n",
      "490 ARI 0.65804, NMI 0.56256\n",
      "500 ARI 0.66348, NMI 0.5679\n",
      "510 ARI 0.66456, NMI 0.56877\n",
      "520 ARI 0.67224, NMI 0.57695\n",
      "530 ARI 0.67442, NMI 0.57873\n",
      "540 ARI 0.67996, NMI 0.58525\n",
      "550 ARI 0.68212, NMI 0.58604\n",
      "560 ARI 0.68426, NMI 0.58587\n",
      "570 ARI 0.68103, NMI 0.58513\n",
      "580 ARI 0.68984, NMI 0.59245\n",
      "590 ARI 0.68653, NMI 0.58969\n",
      "600 ARI 0.6887, NMI 0.59053\n",
      "610 ARI 0.69102, NMI 0.59541\n",
      "620 ARI 0.69431, NMI 0.59717\n",
      "630 ARI 0.69431, NMI 0.59717\n",
      "640 ARI 0.69653, NMI 0.59904\n",
      "650 ARI 0.69427, NMI 0.59617\n",
      "660 ARI 0.69983, NMI 0.60087\n",
      "670 ARI 0.69754, NMI 0.59703\n",
      "680 ARI 0.7021, NMI 0.60375\n",
      "690 ARI 0.69868, NMI 0.59894\n",
      "700 ARI 0.70657, NMI 0.60757\n",
      "710 ARI 0.69976, NMI 0.59893\n",
      "720 ARI 0.71225, NMI 0.6144\n",
      "730 ARI 0.70433, NMI 0.60566\n",
      "740 ARI 0.71109, NMI 0.61242\n",
      "750 ARI 0.70664, NMI 0.60959\n",
      "760 ARI 0.7156, NMI 0.61632\n",
      "770 ARI 0.70885, NMI 0.61049\n",
      "780 ARI 0.71109, NMI 0.61242\n",
      "790 ARI 0.71, NMI 0.61247\n",
      "800 ARI 0.71106, NMI 0.61143\n",
      "810 ARI 0.71334, NMI 0.61436\n",
      "820 ARI 0.71899, NMI 0.61927\n",
      "830 ARI 0.71679, NMI 0.61931\n",
      "840 ARI 0.71676, NMI 0.6183\n",
      "850 ARI 0.71796, NMI 0.62133\n",
      "860 ARI 0.72012, NMI 0.62026\n",
      "870 ARI 0.72242, NMI 0.62323\n",
      "880 ARI 0.72466, NMI 0.62424\n",
      "890 ARI 0.71905, NMI 0.62127\n",
      "900 ARI 0.72235, NMI 0.62127\n",
      "910 ARI 0.72132, NMI 0.62325\n",
      "920 ARI 0.72456, NMI 0.62139\n",
      "930 ARI 0.73042, NMI 0.63125\n",
      "940 ARI 0.72683, NMI 0.62342\n",
      "950 ARI 0.72931, NMI 0.63125\n",
      "960 ARI 0.72807, NMI 0.62726\n",
      "970 ARI 0.73048, NMI 0.63329\n",
      "980 ARI 0.7327, NMI 0.63328\n",
      "990 ARI 0.73391, NMI 0.63632\n",
      "1000 ARI 0.73133, NMI 0.62571\n",
      "1010 ARI 0.73159, NMI 0.63327\n",
      "1020 ARI 0.73042, NMI 0.63125\n",
      "1030 ARI 0.73159, NMI 0.63327\n",
      "1040 ARI 0.7361, NMI 0.63536\n",
      "1050 ARI 0.73499, NMI 0.63532\n",
      "1060 ARI 0.73594, NMI 0.63073\n",
      "1070 ARI 0.73156, NMI 0.63226\n",
      "1080 ARI 0.73384, NMI 0.63429\n",
      "1090 ARI 0.73616, NMI 0.63734\n",
      "1100 ARI 0.74293, NMI 0.63971\n",
      "1110 ARI 0.74643, NMI 0.6438\n",
      "1120 ARI 0.73728, NMI 0.63737\n",
      "1130 ARI 0.73849, NMI 0.64041\n",
      "1140 ARI 0.73846, NMI 0.63939\n",
      "1150 ARI 0.73942, NMI 0.63475\n",
      "1160 ARI 0.74521, NMI 0.64092\n",
      "1170 ARI 0.74752, NMI 0.64304\n",
      "1180 ARI 0.73846, NMI 0.63939\n",
      "1190 ARI 0.74646, NMI 0.64474\n",
      "1200 ARI 0.74877, NMI 0.64686\n",
      "1210 ARI 0.75215, NMI 0.64734\n",
      "1220 ARI 0.75112, NMI 0.64994\n",
      "1230 ARI 0.74987, NMI 0.64608\n",
      "1240 ARI 0.74768, NMI 0.64773\n",
      "1250 ARI 0.75448, NMI 0.64951\n",
      "1260 ARI 0.75219, NMI 0.64823\n",
      "1270 ARI 0.75451, NMI 0.65039\n",
      "1280 ARI 0.75002, NMI 0.65084\n",
      "1290 ARI 0.75576, NMI 0.65423\n",
      "1300 ARI 0.7568, NMI 0.65169\n",
      "1310 ARI 0.75328, NMI 0.64755\n",
      "1320 ARI 0.75448, NMI 0.64951\n",
      "1330 ARI 0.74306, NMI 0.64354\n",
      "1340 ARI 0.7557, NMI 0.65238\n",
      "1350 ARI 0.74883, NMI 0.64879\n",
      "1360 ARI 0.76147, NMI 0.65609\n",
      "1370 ARI 0.7569, NMI 0.65438\n",
      "1380 ARI 0.7568, NMI 0.65169\n",
      "1390 ARI 0.74999, NMI 0.64985\n",
      "1400 ARI 0.76162, NMI 0.66062\n",
      "1410 ARI 0.75576, NMI 0.65423\n",
      "1420 ARI 0.76144, NMI 0.65524\n",
      "1430 ARI 0.76273, NMI 0.65985\n",
      "1440 ARI 0.75809, NMI 0.6564\n",
      "1450 ARI 0.75341, NMI 0.65114\n",
      "1460 ARI 0.75917, NMI 0.65476\n",
      "1470 ARI 0.76144, NMI 0.65524\n",
      "1480 ARI 0.75803, NMI 0.65455\n",
      "1490 ARI 0.75109, NMI 0.64899\n",
      "1500 ARI 0.75926, NMI 0.65749\n",
      "1510 ARI 0.76495, NMI 0.65859\n",
      "1520 ARI 0.76147, NMI 0.65609\n",
      "1530 ARI 0.76036, NMI 0.65675\n",
      "1540 ARI 0.75347, NMI 0.65304\n",
      "1550 ARI 0.76279, NMI 0.66171\n",
      "1560 ARI 0.76273, NMI 0.65985\n",
      "1570 ARI 0.76615, NMI 0.66055\n",
      "1580 ARI 0.76621, NMI 0.66228\n",
      "1590 ARI 0.76507, NMI 0.66206\n",
      "1600 ARI 0.75929, NMI 0.65844\n",
      "1610 ARI 0.75115, NMI 0.65091\n",
      "1620 ARI 0.76156, NMI 0.65875\n",
      "1630 ARI 0.772, NMI 0.66539\n",
      "1640 ARI 0.76501, NMI 0.66029\n",
      "1650 ARI 0.75693, NMI 0.65531\n",
      "1660 ARI 0.76048, NMI 0.66049\n",
      "1670 ARI 0.75693, NMI 0.65531\n",
      "1680 ARI 0.76039, NMI 0.65765\n",
      "1690 ARI 0.76856, NMI 0.66452\n",
      "1700 ARI 0.76974, NMI 0.66565\n",
      "1710 ARI 0.7639, NMI 0.66096\n",
      "1720 ARI 0.76042, NMI 0.65858\n",
      "1730 ARI 0.75585, NMI 0.65717\n",
      "1740 ARI 0.76159, NMI 0.65967\n",
      "1750 ARI 0.7639, NMI 0.66096\n",
      "1760 ARI 0.77209, NMI 0.66791\n",
      "1770 ARI 0.76504, NMI 0.66117\n",
      "1780 ARI 0.75579, NMI 0.65519\n",
      "1790 ARI 0.75696, NMI 0.65627\n",
      "1800 ARI 0.75812, NMI 0.65735\n",
      "1810 ARI 0.76393, NMI 0.66187\n",
      "1820 ARI 0.76742, NMI 0.66429\n",
      "1830 ARI 0.76498, NMI 0.65943\n",
      "1840 ARI 0.76039, NMI 0.65765\n",
      "1850 ARI 0.7535, NMI 0.65403\n",
      "1860 ARI 0.75237, NMI 0.65396\n",
      "1870 ARI 0.75344, NMI 0.65208\n",
      "1880 ARI 0.76387, NMI 0.66006\n",
      "1890 ARI 0.76736, NMI 0.66253\n",
      "1900 ARI 0.7639, NMI 0.66096\n",
      "1910 ARI 0.75469, NMI 0.6561\n",
      "1920 ARI 0.75815, NMI 0.65832\n",
      "1930 ARI 0.75935, NMI 0.6604\n",
      "1940 ARI 0.76282, NMI 0.66267\n",
      "1950 ARI 0.76267, NMI 0.65807\n",
      "1960 ARI 0.76138, NMI 0.65359\n",
      "1970 ARI 0.76375, NMI 0.65664\n",
      "1980 ARI 0.76285, NMI 0.66366\n",
      "1990 ARI 0.75475, NMI 0.65816\n",
      "2000 ARI 0.74546, NMI 0.64869\n",
      "ARI before: 0.7454576369844247\n",
      "Max nb_features 36\n",
      "ARI after: 0.7454576369844247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:57:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:58:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (3000, 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:58:38] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:10:26] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (3000, 2500)\n",
      ">>>>dataset data_0c_2de_0.1\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 499) (3000, 499)\n",
      "begin the pretraining\n",
      "begin the funetraining\n",
      "ARI 0.95016, NMI 0.89603\n",
      "0 ARI 0.95149, NMI 0.89815\n",
      "ARI before: 0.9514877343424022\n",
      "Max nb_features 80\n",
      "ARI after: 0.9514877343424022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:51] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:15:43] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (3000, 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:15:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:27:08] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (3000, 2500)\n",
      ">>>>dataset data_-1c_3de_0.1\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 499) (3000, 499)\n",
      "begin the pretraining\n",
      "begin the funetraining\n",
      "ARI 0.97252, NMI 0.9459\n",
      "0 ARI 0.97252, NMI 0.9459\n",
      "ARI before: 0.9725213309801892\n",
      "Max nb_features 67\n",
      "ARI after: 0.9725213309801892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:44] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:33:05] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (3000, 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:46:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (3000, 2500)\n",
      ">>>>dataset data_0c_3de_0.05\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 499) (3000, 499)\n",
      "begin the pretraining\n",
      "begin the funetraining\n",
      "ARI 0.05318, NMI 0.04941\n",
      "0 ARI 0.05266, NMI 0.04911\n",
      "10 ARI 0.04547, NMI 0.0468\n",
      "20 ARI 0.04218, NMI 0.04514\n",
      "30 ARI 0.04089, NMI 0.04472\n",
      "40 ARI 0.04232, NMI 0.04585\n",
      "50 ARI 0.04125, NMI 0.04586\n",
      "60 ARI 0.04194, NMI 0.04607\n",
      "70 ARI 0.04098, NMI 0.04545\n",
      "80 ARI 0.04094, NMI 0.04518\n",
      "90 ARI 0.04045, NMI 0.04486\n",
      "100 ARI 0.03996, NMI 0.04477\n",
      "110 ARI 0.04018, NMI 0.04492\n",
      "120 ARI 0.03906, NMI 0.04421\n",
      "130 ARI 0.0389, NMI 0.04383\n",
      "140 ARI 0.03806, NMI 0.04333\n",
      "150 ARI 0.03836, NMI 0.04348\n",
      "160 ARI 0.03811, NMI 0.04355\n",
      "170 ARI 0.0379, NMI 0.04292\n",
      "180 ARI 0.03788, NMI 0.04337\n",
      "190 ARI 0.03787, NMI 0.04314\n",
      "200 ARI 0.03832, NMI 0.04392\n",
      "210 ARI 0.03833, NMI 0.04391\n",
      "220 ARI 0.03777, NMI 0.04383\n",
      "230 ARI 0.03824, NMI 0.04462\n",
      "240 ARI 0.03777, NMI 0.04406\n",
      "ARI before: 0.03776760443180342\n",
      "Max nb_features 41\n",
      "ARI after: 0.03776760443180342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in log2\n",
      "invalid value encountered in log2\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:51:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:53:52] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances (3000, 499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n",
      "overflow encountered in expm1\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:54:49] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:09:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "feature_importances (3000, 2500)\n",
      ">>>>dataset data_-1c_2de_0.1\n",
      "(3000, 2500) (3000, 2500)\n",
      "(3000, 500) (3000, 500)\n",
      "begin the pretraining\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3340f2d78535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         chencluster.pretrain(X, count_X, size_factor, args.batch_size,\n\u001b[0;32m---> 65\u001b[0;31m                              args.pretrain_epoch, args.gpu_option)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         chencluster.funetrain(X, count_X, size_factor, args.batch_size,\n",
      "\u001b[0;32m/workspace/notebooks/deep_clustering/interpretability/others/scziDesk_network.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, X, count_X, size_factor, batch_size, pretrain_epoch, gpu_option)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                     (pre_index + 1) * batch_size)],\n\u001b[1;32m    130\u001b[0m                             self.x_count: count_X[(pre_index * batch_size):(\n\u001b[0;32m--> 131\u001b[0;31m                                     (pre_index + 1) * batch_size)]})\n\u001b[0m\u001b[1;32m    132\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_repre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_index\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mpre_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = \"scziDesk\"\n",
    "nb_features = -1\n",
    "pval_cutoff=None\n",
    "run = 0\n",
    "for category in [ \"imbalanced_data\",  \"balanced_data\"\n",
    "                ]:\n",
    "\n",
    "    path= \"..\"\n",
    "    if category in [\"imbalanced_data\", \"balanced_data\"]:\n",
    "        files = glob2.glob(f'{path}/R/simulated_data/{category}/*.h5')\n",
    "        files = [f[len(f\"{path}/R/simulated_data/{category}/\"):-3] for f in files][::-1]\n",
    "    else:\n",
    "        files = glob2.glob(f'{path}/real_data/*.h5')\n",
    "        files = [f[len(f\"{path}/real_data/\"):-3] for f in files]\n",
    "    print(files)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns = [\"dataset\", \"ARI\", \"NMI\", \"sil\", \"run\", \"time\", \"pred\", \"cal\", \"features\"])\n",
    "    for dataset in files:\n",
    "        if category in [\"imbalanced_data\", \"balanced_data\"]:\n",
    "            data_mat = h5py.File(f\"{path}/R/simulated_data/{category}/{dataset}.h5\",\"r\")\n",
    "        else:\n",
    "            data_mat = h5py.File(f\"{path}/real_data/{dataset}.h5\",\"r\")\n",
    "\n",
    "        Y = np.array(data_mat['Y'])\n",
    "        X = np.array(data_mat['X'])\n",
    "        print(f\">>>>dataset {dataset}\")\n",
    "        if X.shape[0] > 10000:\n",
    "            continue\n",
    "\n",
    "        X = np.ceil(X).astype(np.int)\n",
    "        count_X = X\n",
    "        print(X.shape, count_X.shape)\n",
    "        orig_X = X.copy()\n",
    "        adata = sc.AnnData(X)\n",
    "        adata.obs['Group'] = Y\n",
    "        adata = normalize(adata,\n",
    "                          copy=True,\n",
    "                          highly_genes=args.highly_genes,\n",
    "                          size_factors=True,\n",
    "                          normalize_input=True,\n",
    "                          logtrans_input=True)\n",
    "        X = adata.X.astype(np.float32)\n",
    "        Y = np.array(adata.obs[\"Group\"])\n",
    "\n",
    "        high_variable = np.array(adata.var.highly_variable.index, dtype=np.int)\n",
    "        count_X = count_X[:, high_variable]\n",
    "        size_factor = np.array(adata.obs.size_factors).reshape(-1,\n",
    "                                                               1).astype(np.float32)\n",
    "        cluster_number = int(max(Y) - min(Y) + 1)\n",
    "        args.dims[0]=X.shape[1]\n",
    "        print(X.shape, count_X.shape)\n",
    "        idx = adata.var_names.astype(int).tolist()\n",
    "\n",
    "        start = time.time()\n",
    "        seed = run\n",
    "        np.random.seed(seed)\n",
    "        tf.reset_default_graph()\n",
    "        chencluster = autoencoder(args.dataname, args.distribution,\n",
    "                                  args.self_training, args.dims, cluster_number,\n",
    "                                  args.t_alpha, args.alpha, args.gamma,\n",
    "                                  args.learning_rate, args.noise_sd)\n",
    "\n",
    "        chencluster.pretrain(X, count_X, size_factor, args.batch_size,\n",
    "                             args.pretrain_epoch, args.gpu_option)\n",
    "\n",
    "        chencluster.funetrain(X, count_X, size_factor, args.batch_size,\n",
    "                              args.funetrain_epoch, args.update_epoch, args.error, Y)\n",
    "\n",
    "\n",
    "        clusters = chencluster.Y_pred\n",
    "\n",
    "        interpret_utils.de_analysis([X,  np.array(data_mat['X'])],\n",
    "                                    [\"proc_\", \"full_\"],\n",
    "                                    data_mat,\n",
    "                                    idx,\n",
    "                                    method,\n",
    "                                    dataset,\n",
    "                                    category,\n",
    "                                    clusters,\n",
    "                                    nb_features=nb_features,\n",
    "                                    run=run,\n",
    "                                    pval_cutoff=pval_cutoff)\n",
    "        folder = f\"../output/interpretability/{category}/{method}\"\n",
    "        write_to = f\"{folder}/{dataset}\"\n",
    "\n",
    "        start = time.time()\n",
    "        with open(f\"{write_to}_all.pkl\", 'rb') as f:\n",
    "            results= pickle.load(f)\n",
    "        evaluated_gradients = chencluster.gradients\n",
    "\n",
    "        results[\"features\"][\"saliency\"] = []\n",
    "        results[\"features\"][\"grad_x_input\"] = []\n",
    "        stot = pd.DataFrame()\n",
    "        grad_time = 0\n",
    "        for c in np.sort(np.unique(results[\"meta\"][\"clusters\"])):\n",
    "            ii = np.where(results[\"meta\"][\"clusters\"] ==c)[0]\n",
    "        #     g1 = evaluated_gradients[ii] * adata.X[ii]\n",
    "        #     g1 = g1.mean(axis = 0)\n",
    "            g1 = evaluated_gradients[ii].mean(axis = 0)\n",
    "            t1 = time.time()\n",
    "            grad_x_input = evaluated_gradients[ii] * adata.X[ii]\n",
    "            grad_x_input = grad_x_input.mean(0)\n",
    "            t2 = time.time()\n",
    "            grad_time += (t2-t1)\n",
    "            saliency = np.abs(g1)\n",
    "        #     saliency = g1\n",
    "\n",
    "            s_r= np.argsort(saliency)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"saliency\"].append(s_r)\n",
    "            s = pd.DataFrame()\n",
    "            s[\"x\"] = s_r\n",
    "            s[\"saliency\"] = np.sort(saliency)[::-1][:nb_features]\n",
    "            s[\"cluster\"] = c\n",
    "#             print(\">> saliency \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], s_r)))\n",
    "            t1 = time.time()\n",
    "            gi_r= np.argsort(grad_x_input)[::-1][:nb_features].astype(str)\n",
    "            results[\"features\"][\"grad_x_input\"].append(gi_r)\n",
    "            gi = pd.DataFrame()\n",
    "            gi[\"x\"] = gi_r\n",
    "            gi[\"grad_x_input\"] = np.sort(grad_x_input)[::-1][:nb_features]\n",
    "            gi[\"cluster\"] = c\n",
    "            t2 =time.time()\n",
    "            grad_time += (t2-t1)\n",
    "        end = time.time()\n",
    "        results[\"time\"][f\"saliency\"] = end - start - grad_time\n",
    "        results[\"time\"][f\"grad_x_input\"] = end - start\n",
    "#             print(\">> gi \", len(np.intersect1d(results[\"features\"][\"truth\"][int(c)], gi_r)))\n",
    "        s[\"rank\"] = s.groupby(\"cluster\")[\"saliency\"].rank(\"dense\", ascending=False)\n",
    "        gi[\"rank\"] = gi.groupby(\"cluster\")[\"grad_x_input\"].rank(\"dense\", ascending=False)\n",
    "        results[\"scores\"][\"saliency\"] = s\n",
    "        results[\"scores\"][\"grad_x_input\"] = gi\n",
    "        with open(f\"{write_to}_all.pkl\", 'wb') as f:\n",
    "            pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
